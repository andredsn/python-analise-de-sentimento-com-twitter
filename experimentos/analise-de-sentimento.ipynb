{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "\n",
    "#importando os modulos necessarios e criando as funções para preparar os dados\n",
    "\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "from nltk import FreqDist\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def lerCSV(nomeArquivo):\n",
    "    return pd.read_csv(nomeArquivo, encoding='ISO-8859-1', sep=\";\", header=0)\n",
    "\n",
    "def removerPontuacao(texto):\n",
    "    return re.sub(u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', ' ', texto)\n",
    "\n",
    "def removerStopWord(texto):\n",
    "    stopWord=nltk.corpus.stopwords.words('portuguese')\n",
    "    sentencaSemStopword = [i for i in texto.split() if not i in stopWord]\n",
    "    return \" \".join(sentencaSemStopword)\n",
    "\n",
    "def removerAssentuacao(texto):\n",
    "    return normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def aplicarTokenize(texto):\n",
    "    return tokenize.word_tokenize(texto, language='portuguese')\n",
    "\n",
    "def removerNumeros(texto):\n",
    "    return re.sub('[0-9]', '', texto)\n",
    "\n",
    "def removerURL(texto):\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*','', texto)\n",
    "\n",
    "def transformarEmMinusculas(texto):\n",
    "    return texto.lower()\n",
    "\n",
    "def aplicarStemming(texto):\n",
    "    texto=str(texto)\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    palavras=[]\n",
    "    for txt in texto.split():\n",
    "        palavra=stemmer.stem(txt)\n",
    "        palavras.append(palavra)\n",
    "    return \" \".join(palavras)\n",
    "\n",
    "def removerColuna(dataframe):\n",
    "    dataframe=dataframe.drop(dataframe.columns[0], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def removerMensionamento(texto):\n",
    "    if texto.find(\"@\")>=0:\n",
    "        texto=texto.split(\"@\")\n",
    "        texto=str(texto[-1])\n",
    "        contem=texto.find(\" \")\n",
    "        texto=texto[contem+1:]\n",
    "        \n",
    "    return texto\n",
    "\n",
    "def etl(textos):\n",
    "    \n",
    "    tweets=[]\n",
    "   \n",
    "    #trata o texto de cada linha do dataframe\n",
    "    for texto in textos:\n",
    "        texto=str(texto)\n",
    "        \n",
    "        #remover mensionamento (@usuario)\n",
    "        texto=removerMensionamento(texto)\n",
    "        \n",
    "        #remove URL\n",
    "        texto=removerURL(texto)\n",
    "        \n",
    "        #transforma a frase toda em minúscula\n",
    "        texto=transformarEmMinusculas(texto)\n",
    "        \n",
    "        #remove números\n",
    "        texto=removerNumeros(texto)\n",
    "        \n",
    "        #remove as pontuações\n",
    "        texto=removerPontuacao(texto)\n",
    "        \n",
    "        #retirar assentos\n",
    "        texto=removerAssentuacao(texto)\n",
    "        \n",
    "        #aplica stemming\n",
    "        texto=aplicarStemming(texto)\n",
    "        \n",
    "        #remove stopwords\n",
    "        texto=removerStopWord(texto)\n",
    "        \n",
    "        texto=str(texto)\n",
    "        \n",
    "        tweets.append(texto)\n",
    "    return tweets\n",
    "\n",
    "def classificarDecisionTree(texto, sentimento):\n",
    "    print(\"\\nAlgoritmo decision tree\")\n",
    "    \n",
    "    #cria um vetor de 1 palavra\n",
    "    vetor=criarVetor1Palavra()\n",
    "    \n",
    "    #pega a frequência das palavras\n",
    "    texto_freq=vetor.fit_transform(texto)\n",
    "    \n",
    "    #cria o modelo\n",
    "    modelo = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    # Criando uma Confusion Matrix\n",
    "    avaliarModelo(modelo, texto_freq, sentimento)\n",
    "\n",
    "def classificarSVM(texto, sentimento):\n",
    "    print(\"\\nAlgoritmo svm:\")\n",
    "    \n",
    "    #cria um vetor de 1 palavra\n",
    "    vetor = criarVetor1Palavra()\n",
    "    \n",
    "    #pega a frequência das palavras\n",
    "    texto_freq = vetor.fit_transform(texto)\n",
    "    \n",
    "    #cria o modelo\n",
    "    modelo = svm.SVC(gamma=0.001, C=100.)\n",
    "    \n",
    "    # Criando uma Confusion Matrix\n",
    "    avaliarModelo(modelo, texto_freq, sentimento)\n",
    "\n",
    "def classificarMultinomialNB(texto, sentimento):\n",
    "    print(\"\\nAlgoritmo NaiveBayes:\")\n",
    "    \n",
    "        #cria um vetor de 1 palavra\n",
    "    vetor = CountVectorizer(analyzer=\"word\")\n",
    "    \n",
    "    #pega a frequência das palavras\n",
    "    texto_freq = vetor.fit_transform(texto)\n",
    "    \n",
    "    #cria o modelo\n",
    "    modelo = MultinomialNB()\n",
    "    \n",
    "    # Criando uma Confusion Matrix\n",
    "    avaliarModelo(modelo, texto_freq, sentimento)\n",
    "\n",
    "def criarVetor1Palavra():\n",
    "        #a linha abaixo traz o vetor de 1 palavra\n",
    "    return CountVectorizer(analyzer=\"word\")\n",
    "\n",
    "def avaliarModelo(modelo, texto, sentimento):\n",
    "    resultados = cross_val_predict(modelo, texto, sentimento, cv=10)\n",
    "    \n",
    "    #calcula acurácia\n",
    "    acuracia=metrics.accuracy_score(sentimento, resultados)    \n",
    "    print(\"acurácia (cross validation): {:.2f}\".format(acuracia))\n",
    "    print(\"matriz de confusão\")\n",
    "    print(\"INSEGURO - OUTRO\")\n",
    "    print(\"{}\".format(metrics.confusion_matrix(sentimento, resultados.ravel())))\n",
    "\n",
    "def classeNumerica(df):\n",
    "    sentimento_map = {\"inseguro\": 0, \"outro\": 1}\n",
    "    return df.map(sentimento_map)\n",
    "\n",
    "def classificarLogisticRegression(texto, sentimento):\n",
    "    print(\"\\nAlgoritmo regressão logística:\")\n",
    "    \n",
    "        #cria um vetor de 1 palavra\n",
    "    vetor = criarVetor1Palavra()\n",
    "    \n",
    "    #pega a frequência das palavras\n",
    "    texto_freq = vetor.fit_transform(texto)\n",
    "    \n",
    "    #cria o modelo\n",
    "    modelo=LogisticRegression()\n",
    "    \n",
    "    # Criando uma Confusion Matrix\n",
    "    avaliarModelo(modelo, texto_freq, sentimento)\n",
    "\n",
    "def classificarRandomForestClassifier(texto, sentimento):\n",
    "    print(\"\\nAlgoritmo floresta aleatória:\")\n",
    "    \n",
    "    #cria um vetor de 1 palavra\n",
    "    vetor=criarVetor1Palavra()\n",
    "    \n",
    "    #pega a frequência das palavras\n",
    "    texto_freq=vetor.fit_transform(texto)\n",
    "    \n",
    "    #cria o modelo\n",
    "    modelo = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "    # Criando uma Confusion Matrix\n",
    "    avaliarModelo(modelo, texto_freq, sentimento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algoritmo svm:\n",
      "acurácia (cross validation): 0.67\n",
      "matriz de confusão\n",
      "INSEGURO - OUTRO\n",
      "[[149  51]\n",
      " [ 82 118]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #base de tweets classificados\n",
    "    nomeArquivo=\"tweets_classificados.csv\"\n",
    "    \n",
    "    #ler a base de tweets classificados\n",
    "    df=lerCSV(nomeArquivo)\n",
    "    \n",
    "    # pega 200 inseguros e 200 outros\n",
    "    inseguros=df[df['sentimento']==\"inseguro\"]\n",
    "    inseguros=inseguros.head(200)\n",
    "    outros=df[df['sentimento']==\"outro\"]\n",
    "    outros=outros.head(200)\n",
    "    \n",
    "    #concatena os dois dataframes\n",
    "    dfn=inseguros.append(outros, ignore_index=True)\n",
    "    \n",
    "    #remove a coluna usuário do dataframe\n",
    "    dft=removerColuna(dfn)\n",
    "    \n",
    "    #transforma a coluna sentimento de string para número\n",
    "    dft['sentimento']=classeNumerica(dft['sentimento'])\n",
    "    \n",
    "    #trata os textos\n",
    "    dft['texto']=etl(dft['texto'])\n",
    "    \n",
    "    classificarSVM(dft['texto'], dft['sentimento'])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algoritmo decision tree\n",
      "acurácia (cross validation): 0.59\n",
      "matriz de confusão\n",
      "INSEGURO - OUTRO\n",
      "[[137  63]\n",
      " [ 99 101]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #base de tweets classificados\n",
    "    nomeArquivo=\"tweets_classificados.csv\"\n",
    "    \n",
    "    #ler a base de tweets classificados\n",
    "    df=lerCSV(nomeArquivo)\n",
    "    \n",
    "    # pega 200 inseguros e 200 outros\n",
    "    inseguros=df[df['sentimento']==\"inseguro\"]\n",
    "    inseguros=inseguros.head(200)\n",
    "    outros=df[df['sentimento']==\"outro\"]\n",
    "    outros=outros.head(200)\n",
    "    \n",
    "    #concatena os dois dataframes\n",
    "    dfn=inseguros.append(outros, ignore_index=True)\n",
    "    \n",
    "    #remove a coluna usuário do dataframe\n",
    "    dft=removerColuna(dfn)\n",
    "    \n",
    "    #transforma a coluna sentimento de string para número\n",
    "    dft['sentimento']=classeNumerica(dft['sentimento'])\n",
    "    \n",
    "    #trata os textos\n",
    "    dft['texto']=etl(dft['texto'])\n",
    "    \n",
    "    classificarDecisionTree(dft['texto'], dft['sentimento'])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algoritmo NaiveBayes:\n",
      "acurácia (cross validation): 0.61\n",
      "matriz de confusão\n",
      "INSEGURO - OUTRO\n",
      "[[151  49]\n",
      " [105  95]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #base de tweets classificados\n",
    "    nomeArquivo=\"tweets_classificados.csv\"\n",
    "    \n",
    "    #ler a base de tweets classificados\n",
    "    df=lerCSV(nomeArquivo)\n",
    "    \n",
    "    # pega 200 inseguros e 200 outros\n",
    "    inseguros=df[df['sentimento']==\"inseguro\"]\n",
    "    inseguros=inseguros.head(200)\n",
    "    outros=df[df['sentimento']==\"outro\"]\n",
    "    outros=outros.head(200)\n",
    "    \n",
    "    #concatena os dois dataframes\n",
    "    dfn=inseguros.append(outros, ignore_index=True)\n",
    "    \n",
    "    #remove a coluna usuário do dataframe\n",
    "    dft=removerColuna(dfn)\n",
    "    \n",
    "    #transforma a coluna sentimento de string para número\n",
    "    dft['sentimento']=classeNumerica(dft['sentimento'])\n",
    "    \n",
    "    #trata os textos\n",
    "    dft['texto']=etl(dft['texto'])\n",
    "    \n",
    "    classificarMultinomialNB(dft['texto'], dft['sentimento'])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algoritmo regressão logística:\n",
      "acurácia (cross validation): 0.67\n",
      "matriz de confusão\n",
      "INSEGURO - OUTRO\n",
      "[[151  49]\n",
      " [ 84 116]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #base de tweets classificados\n",
    "    nomeArquivo=\"tweets_classificados.csv\"\n",
    "    \n",
    "    #ler a base de tweets classificados\n",
    "    df=lerCSV(nomeArquivo)\n",
    "    \n",
    "    # pega 200 inseguros e 200 outros\n",
    "    inseguros=df[df['sentimento']==\"inseguro\"]\n",
    "    inseguros=inseguros.head(200)\n",
    "    outros=df[df['sentimento']==\"outro\"]\n",
    "    outros=outros.head(200)\n",
    "    \n",
    "    #concatena os dois dataframes\n",
    "    dfn=inseguros.append(outros, ignore_index=True)\n",
    "    \n",
    "    #remove a coluna usuário do dataframe\n",
    "    dft=removerColuna(dfn)\n",
    "    \n",
    "    #transforma a coluna sentimento de string para número\n",
    "    dft['sentimento']=classeNumerica(dft['sentimento'])\n",
    "    \n",
    "    #trata os textos\n",
    "    dft['texto']=etl(dft['texto'])\n",
    "    \n",
    "    classificarLogisticRegression(dft['texto'], dft['sentimento'])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Algoritmo floresta aleatória:\n",
      "acurácia (cross validation): 0.65\n",
      "matriz de confusão\n",
      "INSEGURO - OUTRO\n",
      "[[138  62]\n",
      " [ 77 123]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #base de tweets classificados\n",
    "    nomeArquivo=\"tweets_classificados.csv\"\n",
    "    \n",
    "    #ler a base de tweets classificados\n",
    "    df=lerCSV(nomeArquivo)\n",
    "    \n",
    "    # pega 200 inseguros e 200 outros\n",
    "    inseguros=df[df['sentimento']==\"inseguro\"]\n",
    "    inseguros=inseguros.head(200)\n",
    "    outros=df[df['sentimento']==\"outro\"]\n",
    "    outros=outros.head(200)\n",
    "    \n",
    "    #concatena os dois dataframes\n",
    "    dfn=inseguros.append(outros, ignore_index=True)\n",
    "    \n",
    "    #remove a coluna usuário do dataframe\n",
    "    dft=removerColuna(dfn)\n",
    "    \n",
    "    #transforma a coluna sentimento de string para número\n",
    "    dft['sentimento']=classeNumerica(dft['sentimento'])\n",
    "    \n",
    "    #trata os textos\n",
    "    dft['texto']=etl(dft['texto'])\n",
    "    \n",
    "    classificarRandomForestClassifier(dft['texto'], dft['sentimento'])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
